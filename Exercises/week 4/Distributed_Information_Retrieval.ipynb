{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqClh2tJcRms"
   },
   "source": [
    "## ðŸ“š Exercise 2: Distributed Information Retrieval\n",
    "In this exercise, we implement a core process of Fagin's algorithm, the parallel scanning of the posting lists.\n",
    "Assume an aggregation function that returns the sum of the tf-idf scores given the terms in a document. We assume that the posting lists for each term of a retrieval system are running on a different node.\n",
    "### Use case:\n",
    "You are an engineer who is building a search engine for the recipe finder application _bestbakesinthebuilding.com_. In this application, users can search for their favorite recipe, ingredients, or any recipe-relevant term. The search engine should return relevant recipes that are stored in the website's DB. For the implementation of the searching functionality, you have to use Fagin's retrieval method.\n",
    "\n",
    "To build your system, you have available recipe data in the file bread.txt which is a dump from the application's DB.\n",
    "\n",
    "The schema of the data is the following:\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "### Goal:\n",
    "- Implement the Fagin's algorithm\n",
    "- Test the implementation on `epfldocs.txt` data.\n",
    "\n",
    "### What you are learning in this exercise:\n",
    "- Learn how the Fagin's algorithm (as a distributed retrieval system) works and implement it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T14:05:59.917489Z",
     "start_time": "2022-10-26T14:05:58.722452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunyukang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sunyukang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocess the input text by tokenizing it and stemming it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T14:05:59.939991Z",
     "start_time": "2022-10-26T14:05:59.932638Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NFhRavg7cRmt",
    "outputId": "8a0208cc-d702-4e0b-a001-a2032ddb6efd"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    It tokenizes and stems an input text.\n",
    "    \n",
    "    :param text: str, with the input text\n",
    "    :return: list, of the tokenized and stemmed tokens.\n",
    "    \"\"\"\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vectorize the preprocessed text by producing the TF/IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T14:06:00.278032Z",
     "start_time": "2022-10-26T14:06:00.256836Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param documents: list of lists of str, with tokenized sentences.\n",
    "    :return: dict with the idf values for each vocabulary word.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It generates the vector for an input document (with normalization).\n",
    "    \n",
    "    :param document: list of str with the tokenized documents.\n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param idf: dict with the idf values for each vocabulary word.\n",
    "    :return: list of floats\n",
    "    \"\"\"\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It creates a vector for the given query.\n",
    "\n",
    "    :param query: str\n",
    "    :param vocabulary: list\n",
    "    :param idf: dict\n",
    "    \"\"\"\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    \"\"\"\n",
    "    It computes the search result (get the topk documents).\n",
    "    \n",
    "    :param query: str\n",
    "    :param topk: int\n",
    "    \"\"\"\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPyiybsjcRmx"
   },
   "source": [
    "#### 3. Create a dictionary $h$, where each entry of the dictionary corresponds to a posting list for a term, assumed to be hosted in a different node.\n",
    "(Explore the structure of $h$, to understand it. We suggest to first use the simple collection breads.txt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T14:09:26.856944Z",
     "start_time": "2022-10-26T14:09:26.848069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.45814536593707755,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.11157177565710488,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8047189562170501],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.45814536593707755,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8047189562170501,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8047189562170501,\n",
       "  0.0],\n",
       " [1.6094379124341003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.6094379124341003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.6094379124341003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22314355131420976,\n",
       "  1.6094379124341003,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  1.6094379124341003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  1.6094379124341003,\n",
       "  0.22314355131420976,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.45814536593707755,\n",
       "  0.8047189562170501,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8047189562170501,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.11157177565710488,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T19:10:34.303264Z",
     "start_time": "2022-10-26T19:10:34.295959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 0.8047189562170501), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha.items()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T14:06:01.461313Z",
     "start_time": "2022-10-26T14:06:01.452919Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CPvJvqxWcRmy"
   },
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "doc_vecs = np.transpose(np.array(document_vectors))\n",
    "h = {}\n",
    "for i, term in enumerate(vocabulary):\n",
    "    ha = {}\n",
    "    for docj in range(len(original_documents)):\n",
    "        tfidf = doc_vecs[i][docj]\n",
    "        ha[docj] = tfidf\n",
    "    sorted_ha = sorted(ha.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    h[term] = sorted_ha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T14:11:44.541540Z",
     "start_time": "2022-10-26T14:11:44.533704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.91629073, 0.        , 0.        , 0.91629073, 0.        ],\n",
       "       [0.        , 0.45814537, 0.        , 0.        , 0.45814537],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.80471896],\n",
       "       [0.45814537, 0.        , 0.        , 0.91629073, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.60943791, 0.        ],\n",
       "       [0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.80471896],\n",
       "       [0.        , 0.80471896, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.91629073, 0.91629073],\n",
       "       [0.        , 0.91629073, 0.        , 0.91629073, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.60943791, 0.        ],\n",
       "       [0.11157178, 0.        , 0.22314355, 0.22314355, 0.11157178],\n",
       "       [0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.        , 0.80471896, 0.        , 0.        , 0.        ],\n",
       "       [0.80471896, 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T14:06:40.387222Z",
     "start_time": "2022-10-26T14:06:40.371096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art': [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)],\n",
       " 'bake': [(0, 0.9162907318741551),\n",
       "  (3, 0.9162907318741551),\n",
       "  (1, 0.0),\n",
       "  (2, 0.0),\n",
       "  (4, 0.0)],\n",
       " 'best': [(1, 0.45814536593707755),\n",
       "  (4, 0.45814536593707755),\n",
       "  (0, 0.0),\n",
       "  (2, 0.0),\n",
       "  (3, 0.0)],\n",
       " 'book': [(4, 0.8047189562170501), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0)],\n",
       " 'bread': [(3, 0.9162907318741551),\n",
       "  (0, 0.45814536593707755),\n",
       "  (1, 0.0),\n",
       "  (2, 0.0),\n",
       "  (4, 0.0)],\n",
       " 'cake': [(3, 1.6094379124341003), (0, 0.0), (1, 0.0), (2, 0.0), (4, 0.0)],\n",
       " 'comput': [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)],\n",
       " 'french': [(4, 0.8047189562170501), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0)],\n",
       " 'london': [(1, 0.8047189562170501), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)],\n",
       " 'numer': [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)],\n",
       " 'pastri': [(3, 0.9162907318741551),\n",
       "  (4, 0.9162907318741551),\n",
       "  (0, 0.0),\n",
       "  (1, 0.0),\n",
       "  (2, 0.0)],\n",
       " 'pie': [(1, 0.9162907318741551),\n",
       "  (3, 0.9162907318741551),\n",
       "  (0, 0.0),\n",
       "  (2, 0.0),\n",
       "  (4, 0.0)],\n",
       " 'quantiti': [(3, 1.6094379124341003), (0, 0.0), (1, 0.0), (2, 0.0), (4, 0.0)],\n",
       " 'recip': [(2, 0.22314355131420976),\n",
       "  (3, 0.22314355131420976),\n",
       "  (0, 0.11157177565710488),\n",
       "  (4, 0.11157177565710488),\n",
       "  (1, 0.0)],\n",
       " 'scientif': [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)],\n",
       " 'smith': [(1, 0.8047189562170501), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)],\n",
       " 'without': [(0, 0.8047189562170501), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54S-YarVcRm1"
   },
   "source": [
    "#### 4. Complete the following function that implements the Fagin algorithm.\n",
    "\n",
    "Produce your own datasets to explore the behavior of the algorithm. Create a dataset such that for retrieving a 2 term query a total of 6 documents needs to be retrieved in the first phase of the algorithm (as in the example in the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T19:40:22.642553Z",
     "start_time": "2022-10-26T19:40:22.625206Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5UoBYCJYcRm2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def fagin_algorithm(query, h, k, vocabulary):\n",
    "    \"\"\"\n",
    "    It perofrms the Fahin algorithm.\n",
    "\n",
    "    :param quer: str\n",
    "    :param h: dict\n",
    "    :param vocabulary: list\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split and stem the query\n",
    "    q = query.split()\n",
    "    q = set([stemmer.stem(w) for w in q])\n",
    "    query_term_cnt = len(q)\n",
    "    \n",
    "    # select the posting lists for the query terms\n",
    "    posting_lists = {}\n",
    "    for term in q:\n",
    "        if term in h:\n",
    "            posting_lists[term] = h[term]\n",
    "    \n",
    "    max_len = len(documents)\n",
    "    \n",
    "    # Traverse the selected posting lists until we found k documents that appear in ALL posting lists\n",
    "    # This corresponds to phase 1 of Fagin's algorithm.\n",
    "    # As a result you produce a dictionary documents_occurrences, with the document identifiers as keys, \n",
    "    # and the number of documents found as value.\n",
    "    # We stop traversing the posting lists until we have found k documents that appear in ALL posting lists \n",
    "    # of the query terms\n",
    "    \n",
    "    documents_occurrences = {}\n",
    "    l = 0\n",
    "    found_documents = 0\n",
    "    while l < max_len:\n",
    "        for term in q:\n",
    "            d = posting_lists[term][l][0]\n",
    "            if d in documents_occurrences.keys():\n",
    "                documents_occurrences[d] += 1\n",
    "            else:\n",
    "                documents_occurrences[d] = 1\n",
    "            \n",
    "            if documents_occurrences[d] == query_term_cnt:\n",
    "                found_documents += 1\n",
    "        if found_documents == k:\n",
    "            l = max_len + 1\n",
    "            break\n",
    "        else:\n",
    "            l = l+1\n",
    "                \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "                \n",
    "    print(documents_occurrences)\n",
    "        \n",
    "    # Retrieve for the found documents the tfidf values and add them up.\n",
    "    # For simplicity, we do not distinguish the cases whether the values have already been retrieved in the \n",
    "    # previous phase, or whether we use random access to obtain those values\n",
    "    \n",
    "    tfidf = {}\n",
    "    for d in documents_occurrences.keys():\n",
    "        t = 0\n",
    "        for term in q:\n",
    "            t = t + dict(posting_lists[term])[d]\n",
    "            print(dict(posting_lists[term])[d])\n",
    "        tfidf[d] = t\n",
    "        \n",
    "    # Finally we compute the top-k documents and return the answer\n",
    "    \n",
    "    ans = sorted(tfidf.items(), key=lambda x:x[1], reverse = True)[:k]\n",
    "    return ans\n",
    "\n",
    "\n",
    "ans = fagin_algorithm(\"bread recipe\", h, 2, vocabulary)\n",
    "print(ans)\n",
    "for document_id in ans:\n",
    "    print(original_documents[document_id[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Indexing_Probabilistic_Retrieval.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "75fd394e35225182f207b93437350142e41aafd8fa2b11cc1a17258e1fa2f196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
